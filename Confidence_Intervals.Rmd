---
title: "Confidence intervals"
author: "Put your name here"
date: "Put the date here"
output: pdf_document
---
<!-- Please don't mess with the next two lines! -->
\newenvironment{answer}{\definecolor{shadecolor}{RGB}{225, 225, 255}\begin{shaded}}{\end{shaded}}
<!-- Please don't mess with the previous two lines! -->


## Introduction

Sampling variability means that we can never trust a single sample to identify a population parameter exactly. Instead of simply trusting a point estimate, we can look at the entire sampling distribution to create an interval of plausible values, called a confidence interval. Our understanding of standard errors will help us determine how wide we need to make our confidence intervals in order to have some chance of capturing the true population value. Like hypothesis tests, confidence intervals are a form of inference because they use a sample to deduce something about the population.


## Instructions

Presumably, you have already created a new project and downloaded this file into it. Please knit the document and work back and forth between this R Markdown file and the PDF output as you work through this module.

When you are finished with the assignment, knit to PDF one last time, proofread the PDF file **carefully**, export the PDF file to your computer, and then submit your assignment.

Sometimes you will be asked to add your own R code. That will appear in this document as a code chunk with a request for you to add your own code, like so:

```{r}
## Add code here to [do some task]...
```

Be sure to remove the line `## Add code here to [do some task]...` when you have added your own code.

Sometimes you will be asked to type up your thoughts. That will appear in the document as follows:

\begin{answer}
Please write up your answer here.
\end{answer}

Again, please be sure to remove the line "Please write up your answer here" when you have written up your answer. In these areas of the assignment, please use contextually meaningful full sentences/paragraphs (unless otherwise indicated) and proper spelling, grammar, punctuation, etc. This is not R code, but rather a free response section where you talk about your analysis and conclusions. If you need to use some R code as well, you can use inline R code inside the block between `\begin{answer}` and `\end{answer}`, or if you need an R code chunk, please go outside the `answer` block and start a new code chunk.


## Load Packages

We load the standard `mosaic` package. We'll also need the `openintro` package later in the assignment for the `hsb2` data set. Finally, we introduce the `broom` package that cleans up the output of a wide variety of R commands.

```{r, message = FALSE, warning = FALSE}
library(mosaic)
library(openintro)
library(broom)
```

As always, we set the seed so that our results are reproducible.

```{r}
set.seed(11111)
```


## Sample statistics as an estimate of population parameters

All our previous simulations of sampling distributions were based on the assumption that we knew $p$, the true population proportion. For example, we assumed that a candidate in an election actually had 64% support and then deduced various properties of the sampling distribution model.

In reality, however, we never know $p$. In fact, that's the whole point of statistics: we do not know the true population parameters, so we gather samples. We will measure $\hat{p}$, our sample proportion, and we hope that $\hat{p}$ is a good estimate of $p$.

Because of sampling variability, $\hat{p}$ will almost never be exactly the same as $p$, but now we have a tool for figuring out how close together they should be. The sampling distribution model tells us that when we sample a value of $\hat{p}$, it should be within a few standard errors of $p$.


## Estimating the standard error

We do need to make one adjustment to our standard error. The formula we gave in a previous assignment was

$$\sqrt{\frac{p(1 - p)}{n}}.$$

However, in a situation where we have collected data to try to estimate $p$, clearly we do not know $p$. So we plug in the estimate from our sample instead and call this the standard error, which we abbreviate as $SE$:

$$SE = \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}.$$


## Building a confidence interval

Recall that about 95% of a distribution lies within two standard deviations of the mean. Therefore, it should be the case that the true proportion $p$ should lie within two standard errors of $\hat{p}$ most of the time.

This is the idea of a confidence interval. Take $\hat{p}$ from the sample and add/subtract 2 standard errors:

$$\left(\hat{p} -  2 SE, \hat{p} + 2 SE \right)$$

$$ = \left(\hat{p} -  2 \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}, \hat{p} + 2 \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}} \right).$$

### Exercise

The number 2---derived from the 68-95-99.7 rule---is, in fact, slightly incorrect. Using your knowledge of normal models and the `qdist` command, what is the number of standard errors that would enclose exactly 95% of the middle of the sampling distribution? (Hint: `p = 0.95` is **not** the correct argument in the `qdist` function. The middle 95% does not occur at the 95th percentile. If you're still confused, go on and read the next section and revisit this problem later.)

```{r}
## Add code here to calculate the exact number of standard errors 
## from the mean that enclose the middle 95% of a normal distribution.
```

**Important note: remember that if you want to type the percent sign % in the blue answer block, you must precede it with a backslash!**

\begin{answer}
Please write up your answer here.
The percent sign must be typed like this: \%
\end{answer}

*****

The interpretation is that when you go collect a sample, the confidence interval you produce using your estimate $\hat{p}$ will capture the true population proportion 95% of the time.

There is no particular reason that we need to compute a 95% confidence interval, although that is the generally agreed-upon standard. We could compute a 90% confidence interval or a 99% confidence interval, or any other type of interval. (Having said that, if you choose other intervals besides these three, people might wonder if you're up to something.)

The general formula, then, is

$$\left(\hat{p} -  z^{*} \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}, \hat{p} + z^{*} \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}} \right).$$

The new symbol $z^{*}$ is called a *critical z-score*. This is the z-score that encloses the confidence level you want.

For example, suppose we wanted a 90% confidence interval. What is the corresponding critical z-score? Well, the middle 90% of a distribution leaves 5% in each tail. Therefore, we need to use `qdist` with the 5th and 95th percentiles.

```{r}
qdist("norm", p = c(0.05, 0.95))
```

The critical z-score is the positive answer, so we could use `qdist` simply to report `r qdist("norm", p = 0.95)`. **Be careful!** The critical z-score for a 90% confidence interval requires `p = 0.95`. This is because the upper endpoint of a 90% interval is actually located at the 95th percentile.

(Also, if you had trouble with the question above that asked about the exact critical z-score for a 95% confidence interval, go back and try again now that you've seen another worked example.)

### Exercise

Now do the same thing for a 99% confidence interval. In other words, calculate the critical z-score that encloses the middle 99% of the normal distribution.

```{r}
## Add code here to calculate the exact number of standard errors 
## from the mean that enclose the middle 99% of a normal distribution.
```

\begin{answer}
Please write up your answer here.
\end{answer}

*****


## Checking conditions

Don't forget that there are always conditions to check. Before computing a confidence interval for a proportion, you must verify that the conditions are satisfied. These conditions are not really new; essentially, we are just checking the conditions that we already established for using a normal model as a sampling distribution model.

The "Random" and "10%" conditions will always be present. These are both necessary any time you want to infer from a sample.

The success/failure condition is also present, but here it looks a little different. Whereas before we computed $np$ and $n(1 - p)$, that was back when we were assuming we knew the true value of $p$. In reality, this is never the case; if we knew the true value of $p$ already, we wouldn't be gathering a sample to try to infer it! So if we can't compute $np$ and $n(1 - p)$, what can we compute? Since the sample proportion $\hat{p}$ is our best guess for $p$, we will substitute its value everywhere there's a $p$.

So the new success/failure condition says that $n \hat{p}$ and $n(1 - \hat{p})$ must be greater than 10. But, wait a minute, what are we really calculating with $n \hat{p}$ and $n(1 - \hat{p})$? Since $n$ is the sample size, and $\hat{p}$ is the sample proportion of successes, then $n \hat{p}$ is just the number, or raw count, of successes. And similarly, $n(1 - \hat{p})$ is just the number of failures.

Unlike $np$ that describes the "theoretical" average expected number of successes---and therefore might not be a whole number---$n \hat{p}$ is the actual number of successes in our sample. This will always be a whole number! Same with the failures.

As an example, suppose a sample of size 67 has 52 successes. Then our sample proportion is

$$\hat{p} = \frac{52}{67} = 0.776$$

Keep in mind, though, that 0.776 is a rounded estimate. If we tried to compute $n \hat{p} = 67(0.776)$, we get 51.992. That's an absurd thing to do! The quantity $n \hat{p}$ is just the number of successes, and we already know it's 52. **Don't use a rounded (and therefore slightly incorrect) sample proportion to calculate the number of successes and failures. Just use the actual (whole) number of successes and failures.**

Okay, let's formally write down all the conditions that need to be checked:

* Random
    - The sample must be random (or hopefully representative).
* 10%
    - The sample size must be less than 10% of the size of the population.
* Success/failure
    - The number of succeses and the number of failures **in our sample** must both exceed 10.


## Using R to calculate confidence intervals

There are three forms in which you might have categorical data. They all use the command `prop.test`, but in slightly different ways.

### Method 1

You may just have a summary of the total number of successes and failures. For example, suppose we survey 326 people and 212 of them support a new initiative. (This means that 114 do not support it.) Assuming we have a random sample that is less than 10% of the population, and seeing as we have far more than 10 successes and failures, we get the confidence interval as follows:

```{r}
prop.test(x = 212, n = 326)
```

This does give us our confidence interval, but it also gives us a bunch of other stuff we don't need right now. (It's actually doing the "Mechanics" section for a full hypothesis test.) If we want just the confidence interval, the easiest thing to do is use the helpful `broom` package to take all this messy output and tidy it up first. As a matter of fact, the `broom` command that accomplishes this is called `tidy`. We'll assign the output to a variable `test_1` so we can access it.

```{r}
test_1 <- tidy(prop.test(x = 212, n = 326))
test_1
```

For now, ignore everything but the two numbers of interest: the low and high endpoints of our confidence interval. Here is how we report this inline:

We are 95% confident that the true proportion of those who support the new initiative is captured in the interval (`r 100 * test_1$conf.low`%, `r 100 * test_1$conf.high`%).

Notice that when communicating proportions to human beings, it's polite to convert them back to percentages.

### Method 2

If you are given the percentages of successes and/or failures in your data, you'll have to convert them to whole number totals. For example, if we're told that 326 people were surveyed and 65% of them support the new initiative, then we have to do this:

```{r}
test_2 <- tidy(prop.test(x = round(326*0.65), n = 326))
test_2
```

We have to round the number inside this command since 326 times 0.65 is not a whole number. It is the 65% that is, in fact, rounded, but we have no way of knowing what the exact number of successes was if all we're told is the proportion of successes. It is possible that the true number of successes was actually 212, but 211 and 213 also round to 65%.

The confidence interval is identical to the one from Method 1.

### Method 3

Finally, it is possible that we have categorical data in a data frame. For example, what percentage of U.S. high school seniors go to private school? We use the `schtyp` variable in the `hsb2` data set.

First, check the conditions. The sample is presumably a representative sample of high school seniors from the U.S. as the survey was conducted by the National Center of Education Statistics. The sample size is `r NROW(hsb2)`, which is much less than 10% of the population of all U.S. high school seniors. Finally, we look at the number of successes and failures:

```{r}
table(hsb2$schtyp)
```

There are more than 10 successes and more than 10 failures (where a success is defined here to mean a senior who goes to private school).

Now we are ready to compute. If we do the following, though, we get the wrong answer:

```{r}
test_3a <- tidy(prop.test(x = hsb2$schtyp))
test_3a
```

### Exercise

Examine the output above and see if you can spot the problem.

\begin{answer}
Please write up your answer here.
\end{answer}

*****

Try this instead:

```{r}
test_3b <- tidy(prop.test(x = hsb2$schtyp, success = "private"))
test_3b
```

We are 95% confident that the true proportion of U.S. high school seniors who attend private school is captured in the interval (`r 100 * test_3b$conf.low`%, `r 100 * test_3b$conf.high`%).


## Changing the confidence level

The default confidence level for a confidence interval is almost always 95%. It is possible, however, to use a different level.

```{r}
test_3c <- tidy(prop.test(x = hsb2$schtyp, success = "private",
                          conf.level = 0.9))
test_3c
```

### Exercise

Is a 90% confidence interval wider or narrower than a 95% confidence interval? Explain why this is so. (In other words, from your understanding of how confidence intervals work, explain why it makes sense that a 90% confidence interval is wider or narrower than a 95% confidence interval.)

\begin{answer}
Please write up your answer here.
\end{answer}

*****


## Rubric for confidence intervals

Calculating a confidence interval is a form of statistical inference. Typically, you will be asked to report a confidence interval after performing a hypothesis test. Whereas a hypothesis test gives you a "decision criterion" (using data to make a decision to reject the null or fail to reject the null), a confidence interval gives you an estimate of the "effect size" (a range of plausible values for the population parameter).

As such, there is a section in the "Rubric for Inference" that shows the steps of calculating and reporting a confidence interval. They are easy to remember as the "three Cs": Conditions, Calculation, Conclusion.

Here is a worked example.

Some of the students in the "High School and Beyond" survey attended vocational programs. What percentage of all high school seniors attend vocational programs?

### Conditions

* Random
    - The sample is presumably a representative sample of high school seniors from the U.S. as the survey was conducted by the National Center of Education Statistics.

* 10%
    - The sample size is `r NROW(hsb2)`, which is much less than 10% of the population of all U.S. high school seniors.

* Success/failure

```{r}
table(hsb2$prog)
```

The number of "successes" (students in vocational programs) is 50, which is more than 10, and the number of "failures" (all other programs) is 150, also more than 10.

### Calculation

```{r}
program <- tidy(prop.test(x = hsb2$prog, success = "vocational"))
program
```

### Conclusion

We are 95% confident that the true proportion of U.S. high school seniors who attend a vocational program is captured in the interval (`r 100 * program$conf.low`%, `r 100 * program$conf.high`%).


## Your turn

Use the `smoking` data set from the `openintro` package. What percentage of the population of the U.K. smokes tobacco? (The information you need is in the `smoke` variable.)

### Conditions

* Random

\begin{answer}
Please write up your answer here.
\end{answer}

* 10%

\begin{answer}
Please write up your answer here.
\end{answer}

* Success/failure

\begin{answer}
Please write up your answer here.
\end{answer}

### Calculation

```{r}
## Add code here to calculate the confidence interval.
```

### Conclusion

\begin{answer}
Please write up your answer here.
\end{answer}


## Conclusion

A confidence interval is a form of statistical inference that gives us a range of numbers in which we hope to capture the true population parameter. Of course, we can't be certain of that. If we repeatedly collect samples, the expectation is that 95% of those samples will capture the true population parameter, but that also means that 5% will not. We'll never know if our sample was one of the 95% that worked, or one of the 5% that did not. Because it's statistics, we just have to live with that uncertainty.
